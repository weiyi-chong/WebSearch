{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwiQMCEJ30BF",
        "outputId": "ac96542f-1575-43fd-ae98-6d38cbaab0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello World. It's good to see you. Thanks for attending this tutorial\"\n",
        "\n",
        "#split paragraph into sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wPEGhFG_Kh0",
        "outputId": "d9f39985-afa8-4134-e0d4-2c6516e593f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello World.', \"It's good to see you.\", 'Thanks for attending this tutorial']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split sentences into tokens\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize('Hello World.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yuZdb9aDFJB",
        "outputId": "6d0952c4-54e4-4d83-9bd5-272f58114faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'World', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"I can't sleep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGL_GJKvDpQR",
        "outputId": "2bba3ce2-1aa3-4d2a-a9e4-fda8fc26bb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'ca', \"n't\", 'sleep']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize(\"I can't sleep.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZabni1eD7pi",
        "outputId": "5a2270fd-44f2-4c30-e553-43ab3c26948c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'can', \"'\", 't', 'sleep', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing using regular expression\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\") #Tokenise when there's non-word character\n",
        "\n",
        "tokenizer.tokenize(\"I can't sleep.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yba8tplqFZlk",
        "outputId": "3b1491e0-a5bb-43d7-9b85-239aa025914c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'can', 't', 'sleep']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.corpus import webtext\n",
        "\n",
        "text = webtext.raw('overheard.txt')\n",
        "sent_tokenizer = PunktSentenceTokenizer(text)\n",
        "sent1 = sent_tokenizer.tokenize(text)\n",
        "sent1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8_E3RdxOGgK5",
        "outputId": "37131e89-a47d-49fd-c72c-8b5470953653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'White guy: So, do you have any plans for this evening?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent1[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SYmb9DmcJKm6",
        "outputId": "6f5a4b1e-3b15-47f7-97e6-dc8578bbf0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'White guy: Oh, that sounds good.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with the previous tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent2 = sent_tokenize(text)\n",
        "sent2[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DNb-RMZTJRXm",
        "outputId": "87a6a316-daf9-4f85-91fe-16b9b00933df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'White guy: So, do you have any plans for this evening?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "english_stops = set(stopwords.words('english'))\n",
        "words = [\"I\",\"can't\", \"sleep\", \"because\", \"I\", \"drink\", \"a\", \"lot\", \"of\",\"coffee\"]"
      ],
      "metadata": {
        "id": "JB_bF4kVK3gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9gBDMpGMI43",
        "outputId": "f81c210b-5a4e-4dee-e4e3-9314e5a08dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('chinese')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwulmkglMQ1e",
        "outputId": "2f785ebc-0ecf-4f98-e414-81bbbc234d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['一',\n",
              " '一下',\n",
              " '一些',\n",
              " '一切',\n",
              " '一则',\n",
              " '一天',\n",
              " '一定',\n",
              " '一方面',\n",
              " '一旦',\n",
              " '一时',\n",
              " '一来',\n",
              " '一样',\n",
              " '一次',\n",
              " '一片',\n",
              " '一直',\n",
              " '一致',\n",
              " '一般',\n",
              " '一起',\n",
              " '一边',\n",
              " '一面',\n",
              " '万一',\n",
              " '上下',\n",
              " '上升',\n",
              " '上去',\n",
              " '上来',\n",
              " '上述',\n",
              " '上面',\n",
              " '下列',\n",
              " '下去',\n",
              " '下来',\n",
              " '下面',\n",
              " '不一',\n",
              " '不久',\n",
              " '不仅',\n",
              " '不会',\n",
              " '不但',\n",
              " '不光',\n",
              " '不单',\n",
              " '不变',\n",
              " '不只',\n",
              " '不可',\n",
              " '不同',\n",
              " '不够',\n",
              " '不如',\n",
              " '不得',\n",
              " '不怕',\n",
              " '不惟',\n",
              " '不成',\n",
              " '不拘',\n",
              " '不敢',\n",
              " '不断',\n",
              " '不是',\n",
              " '不比',\n",
              " '不然',\n",
              " '不特',\n",
              " '不独',\n",
              " '不管',\n",
              " '不能',\n",
              " '不要',\n",
              " '不论',\n",
              " '不足',\n",
              " '不过',\n",
              " '不问',\n",
              " '与',\n",
              " '与其',\n",
              " '与否',\n",
              " '与此同时',\n",
              " '专门',\n",
              " '且',\n",
              " '两者',\n",
              " '严格',\n",
              " '严重',\n",
              " '个',\n",
              " '个人',\n",
              " '个别',\n",
              " '中小',\n",
              " '中间',\n",
              " '丰富',\n",
              " '临',\n",
              " '为',\n",
              " '为主',\n",
              " '为了',\n",
              " '为什么',\n",
              " '为什麽',\n",
              " '为何',\n",
              " '为着',\n",
              " '主张',\n",
              " '主要',\n",
              " '举行',\n",
              " '乃',\n",
              " '乃至',\n",
              " '么',\n",
              " '之',\n",
              " '之一',\n",
              " '之前',\n",
              " '之后',\n",
              " '之後',\n",
              " '之所以',\n",
              " '之类',\n",
              " '乌乎',\n",
              " '乎',\n",
              " '乘',\n",
              " '也',\n",
              " '也好',\n",
              " '也是',\n",
              " '也罢',\n",
              " '了',\n",
              " '了解',\n",
              " '争取',\n",
              " '于',\n",
              " '于是',\n",
              " '于是乎',\n",
              " '云云',\n",
              " '互相',\n",
              " '产生',\n",
              " '人们',\n",
              " '人家',\n",
              " '什么',\n",
              " '什么样',\n",
              " '什麽',\n",
              " '今后',\n",
              " '今天',\n",
              " '今年',\n",
              " '今後',\n",
              " '仍然',\n",
              " '从',\n",
              " '从事',\n",
              " '从而',\n",
              " '他',\n",
              " '他人',\n",
              " '他们',\n",
              " '他的',\n",
              " '代替',\n",
              " '以',\n",
              " '以上',\n",
              " '以下',\n",
              " '以为',\n",
              " '以便',\n",
              " '以免',\n",
              " '以前',\n",
              " '以及',\n",
              " '以后',\n",
              " '以外',\n",
              " '以後',\n",
              " '以来',\n",
              " '以至',\n",
              " '以至于',\n",
              " '以致',\n",
              " '们',\n",
              " '任',\n",
              " '任何',\n",
              " '任凭',\n",
              " '任务',\n",
              " '企图',\n",
              " '伟大',\n",
              " '似乎',\n",
              " '似的',\n",
              " '但',\n",
              " '但是',\n",
              " '何',\n",
              " '何况',\n",
              " '何处',\n",
              " '何时',\n",
              " '作为',\n",
              " '你',\n",
              " '你们',\n",
              " '你的',\n",
              " '使得',\n",
              " '使用',\n",
              " '例如',\n",
              " '依',\n",
              " '依照',\n",
              " '依靠',\n",
              " '促进',\n",
              " '保持',\n",
              " '俺',\n",
              " '俺们',\n",
              " '倘',\n",
              " '倘使',\n",
              " '倘或',\n",
              " '倘然',\n",
              " '倘若',\n",
              " '假使',\n",
              " '假如',\n",
              " '假若',\n",
              " '做到',\n",
              " '像',\n",
              " '允许',\n",
              " '充分',\n",
              " '先后',\n",
              " '先後',\n",
              " '先生',\n",
              " '全部',\n",
              " '全面',\n",
              " '兮',\n",
              " '共同',\n",
              " '关于',\n",
              " '其',\n",
              " '其一',\n",
              " '其中',\n",
              " '其二',\n",
              " '其他',\n",
              " '其余',\n",
              " '其它',\n",
              " '其实',\n",
              " '其次',\n",
              " '具体',\n",
              " '具体地说',\n",
              " '具体说来',\n",
              " '具有',\n",
              " '再者',\n",
              " '再说',\n",
              " '冒',\n",
              " '冲',\n",
              " '决定',\n",
              " '况且',\n",
              " '准备',\n",
              " '几',\n",
              " '几乎',\n",
              " '几时',\n",
              " '凭',\n",
              " '凭借',\n",
              " '出去',\n",
              " '出来',\n",
              " '出现',\n",
              " '分别',\n",
              " '则',\n",
              " '别',\n",
              " '别的',\n",
              " '别说',\n",
              " '到',\n",
              " '前后',\n",
              " '前者',\n",
              " '前进',\n",
              " '前面',\n",
              " '加之',\n",
              " '加以',\n",
              " '加入',\n",
              " '加强',\n",
              " '十分',\n",
              " '即',\n",
              " '即令',\n",
              " '即使',\n",
              " '即便',\n",
              " '即或',\n",
              " '即若',\n",
              " '却不',\n",
              " '原来',\n",
              " '又',\n",
              " '及',\n",
              " '及其',\n",
              " '及时',\n",
              " '及至',\n",
              " '双方',\n",
              " '反之',\n",
              " '反应',\n",
              " '反映',\n",
              " '反过来',\n",
              " '反过来说',\n",
              " '取得',\n",
              " '受到',\n",
              " '变成',\n",
              " '另',\n",
              " '另一方面',\n",
              " '另外',\n",
              " '只是',\n",
              " '只有',\n",
              " '只要',\n",
              " '只限',\n",
              " '叫',\n",
              " '叫做',\n",
              " '召开',\n",
              " '叮咚',\n",
              " '可',\n",
              " '可以',\n",
              " '可是',\n",
              " '可能',\n",
              " '可见',\n",
              " '各',\n",
              " '各个',\n",
              " '各人',\n",
              " '各位',\n",
              " '各地',\n",
              " '各种',\n",
              " '各级',\n",
              " '各自',\n",
              " '合理',\n",
              " '同',\n",
              " '同一',\n",
              " '同时',\n",
              " '同样',\n",
              " '后来',\n",
              " '后面',\n",
              " '向',\n",
              " '向着',\n",
              " '吓',\n",
              " '吗',\n",
              " '否则',\n",
              " '吧',\n",
              " '吧哒',\n",
              " '吱',\n",
              " '呀',\n",
              " '呃',\n",
              " '呕',\n",
              " '呗',\n",
              " '呜',\n",
              " '呜呼',\n",
              " '呢',\n",
              " '周围',\n",
              " '呵',\n",
              " '呸',\n",
              " '呼哧',\n",
              " '咋',\n",
              " '和',\n",
              " '咚',\n",
              " '咦',\n",
              " '咱',\n",
              " '咱们',\n",
              " '咳',\n",
              " '哇',\n",
              " '哈',\n",
              " '哈哈',\n",
              " '哉',\n",
              " '哎',\n",
              " '哎呀',\n",
              " '哎哟',\n",
              " '哗',\n",
              " '哟',\n",
              " '哦',\n",
              " '哩',\n",
              " '哪',\n",
              " '哪个',\n",
              " '哪些',\n",
              " '哪儿',\n",
              " '哪天',\n",
              " '哪年',\n",
              " '哪怕',\n",
              " '哪样',\n",
              " '哪边',\n",
              " '哪里',\n",
              " '哼',\n",
              " '哼唷',\n",
              " '唉',\n",
              " '啊',\n",
              " '啐',\n",
              " '啥',\n",
              " '啦',\n",
              " '啪达',\n",
              " '喂',\n",
              " '喏',\n",
              " '喔唷',\n",
              " '嗡嗡',\n",
              " '嗬',\n",
              " '嗯',\n",
              " '嗳',\n",
              " '嘎',\n",
              " '嘎登',\n",
              " '嘘',\n",
              " '嘛',\n",
              " '嘻',\n",
              " '嘿',\n",
              " '因',\n",
              " '因为',\n",
              " '因此',\n",
              " '因而',\n",
              " '固然',\n",
              " '在',\n",
              " '在下',\n",
              " '地',\n",
              " '坚决',\n",
              " '坚持',\n",
              " '基本',\n",
              " '处理',\n",
              " '复杂',\n",
              " '多',\n",
              " '多少',\n",
              " '多数',\n",
              " '多次',\n",
              " '大力',\n",
              " '大多数',\n",
              " '大大',\n",
              " '大家',\n",
              " '大批',\n",
              " '大约',\n",
              " '大量',\n",
              " '失去',\n",
              " '她',\n",
              " '她们',\n",
              " '她的',\n",
              " '好的',\n",
              " '好象',\n",
              " '如',\n",
              " '如上所述',\n",
              " '如下',\n",
              " '如何',\n",
              " '如其',\n",
              " '如果',\n",
              " '如此',\n",
              " '如若',\n",
              " '存在',\n",
              " '宁',\n",
              " '宁可',\n",
              " '宁愿',\n",
              " '宁肯',\n",
              " '它',\n",
              " '它们',\n",
              " '它们的',\n",
              " '它的',\n",
              " '安全',\n",
              " '完全',\n",
              " '完成',\n",
              " '实现',\n",
              " '实际',\n",
              " '宣布',\n",
              " '容易',\n",
              " '密切',\n",
              " '对',\n",
              " '对于',\n",
              " '对应',\n",
              " '将',\n",
              " '少数',\n",
              " '尔后',\n",
              " '尚且',\n",
              " '尤其',\n",
              " '就',\n",
              " '就是',\n",
              " '就是说',\n",
              " '尽',\n",
              " '尽管',\n",
              " '属于',\n",
              " '岂但',\n",
              " '左右',\n",
              " '巨大',\n",
              " '巩固',\n",
              " '己',\n",
              " '已经',\n",
              " '帮助',\n",
              " '常常',\n",
              " '并',\n",
              " '并不',\n",
              " '并不是',\n",
              " '并且',\n",
              " '并没有',\n",
              " '广大',\n",
              " '广泛',\n",
              " '应当',\n",
              " '应用',\n",
              " '应该',\n",
              " '开外',\n",
              " '开始',\n",
              " '开展',\n",
              " '引起',\n",
              " '强烈',\n",
              " '强调',\n",
              " '归',\n",
              " '当',\n",
              " '当前',\n",
              " '当时',\n",
              " '当然',\n",
              " '当着',\n",
              " '形成',\n",
              " '彻底',\n",
              " '彼',\n",
              " '彼此',\n",
              " '往',\n",
              " '往往',\n",
              " '待',\n",
              " '後来',\n",
              " '後面',\n",
              " '得',\n",
              " '得出',\n",
              " '得到',\n",
              " '心里',\n",
              " '必然',\n",
              " '必要',\n",
              " '必须',\n",
              " '怎',\n",
              " '怎么',\n",
              " '怎么办',\n",
              " '怎么样',\n",
              " '怎样',\n",
              " '怎麽',\n",
              " '总之',\n",
              " '总是',\n",
              " '总的来看',\n",
              " '总的来说',\n",
              " '总的说来',\n",
              " '总结',\n",
              " '总而言之',\n",
              " '恰恰相反',\n",
              " '您',\n",
              " '意思',\n",
              " '愿意',\n",
              " '慢说',\n",
              " '成为',\n",
              " '我',\n",
              " '我们',\n",
              " '我的',\n",
              " '或',\n",
              " '或是',\n",
              " '或者',\n",
              " '战斗',\n",
              " '所',\n",
              " '所以',\n",
              " '所有',\n",
              " '所谓',\n",
              " '打',\n",
              " '扩大',\n",
              " '把',\n",
              " '抑或',\n",
              " '拿',\n",
              " '按',\n",
              " '按照',\n",
              " '换句话说',\n",
              " '换言之',\n",
              " '据',\n",
              " '掌握',\n",
              " '接着',\n",
              " '接著',\n",
              " '故',\n",
              " '故此',\n",
              " '整个',\n",
              " '方便',\n",
              " '方面',\n",
              " '旁人',\n",
              " '无宁',\n",
              " '无法',\n",
              " '无论',\n",
              " '既',\n",
              " '既是',\n",
              " '既然',\n",
              " '时候',\n",
              " '明显',\n",
              " '明确',\n",
              " '是',\n",
              " '是否',\n",
              " '是的',\n",
              " '显然',\n",
              " '显著',\n",
              " '普通',\n",
              " '普遍',\n",
              " '更加',\n",
              " '曾经',\n",
              " '替',\n",
              " '最后',\n",
              " '最大',\n",
              " '最好',\n",
              " '最後',\n",
              " '最近',\n",
              " '最高',\n",
              " '有',\n",
              " '有些',\n",
              " '有关',\n",
              " '有利',\n",
              " '有力',\n",
              " '有所',\n",
              " '有效',\n",
              " '有时',\n",
              " '有点',\n",
              " '有的',\n",
              " '有着',\n",
              " '有著',\n",
              " '望',\n",
              " '朝',\n",
              " '朝着',\n",
              " '本',\n",
              " '本着',\n",
              " '来',\n",
              " '来着',\n",
              " '极了',\n",
              " '构成',\n",
              " '果然',\n",
              " '果真',\n",
              " '某',\n",
              " '某个',\n",
              " '某些',\n",
              " '根据',\n",
              " '根本',\n",
              " '欢迎',\n",
              " '正在',\n",
              " '正如',\n",
              " '正常',\n",
              " '此',\n",
              " '此外',\n",
              " '此时',\n",
              " '此间',\n",
              " '毋宁',\n",
              " '每',\n",
              " '每个',\n",
              " '每天',\n",
              " '每年',\n",
              " '每当',\n",
              " '比',\n",
              " '比如',\n",
              " '比方',\n",
              " '比较',\n",
              " '毫不',\n",
              " '没有',\n",
              " '沿',\n",
              " '沿着',\n",
              " '注意',\n",
              " '深入',\n",
              " '清楚',\n",
              " '满足',\n",
              " '漫说',\n",
              " '焉',\n",
              " '然则',\n",
              " '然后',\n",
              " '然後',\n",
              " '然而',\n",
              " '照',\n",
              " '照着',\n",
              " '特别是',\n",
              " '特殊',\n",
              " '特点',\n",
              " '现代',\n",
              " '现在',\n",
              " '甚么',\n",
              " '甚而',\n",
              " '甚至',\n",
              " '用',\n",
              " '由',\n",
              " '由于',\n",
              " '由此可见',\n",
              " '的',\n",
              " '的话',\n",
              " '目前',\n",
              " '直到',\n",
              " '直接',\n",
              " '相似',\n",
              " '相信',\n",
              " '相反',\n",
              " '相同',\n",
              " '相对',\n",
              " '相对而言',\n",
              " '相应',\n",
              " '相当',\n",
              " '相等',\n",
              " '省得',\n",
              " '看出',\n",
              " '看到',\n",
              " '看来',\n",
              " '看看',\n",
              " '看见',\n",
              " '真是',\n",
              " '真正',\n",
              " '着',\n",
              " '着呢',\n",
              " '矣',\n",
              " '知道',\n",
              " '确定',\n",
              " '离',\n",
              " '积极',\n",
              " '移动',\n",
              " '突出',\n",
              " '突然',\n",
              " '立即',\n",
              " '第',\n",
              " '等',\n",
              " '等等',\n",
              " '管',\n",
              " '紧接着',\n",
              " '纵',\n",
              " '纵令',\n",
              " '纵使',\n",
              " '纵然',\n",
              " '练习',\n",
              " '组成',\n",
              " '经',\n",
              " '经常',\n",
              " '经过',\n",
              " '结合',\n",
              " '结果',\n",
              " '给',\n",
              " '绝对',\n",
              " '继续',\n",
              " '继而',\n",
              " '维持',\n",
              " '综上所述',\n",
              " '罢了',\n",
              " '考虑',\n",
              " '者',\n",
              " '而',\n",
              " '而且',\n",
              " '而况',\n",
              " '而外',\n",
              " '而已',\n",
              " '而是',\n",
              " '而言',\n",
              " '联系',\n",
              " '能',\n",
              " '能否',\n",
              " '能够',\n",
              " '腾',\n",
              " '自',\n",
              " '自个儿',\n",
              " '自从',\n",
              " '自各儿',\n",
              " '自家',\n",
              " '自己',\n",
              " '自身',\n",
              " '至',\n",
              " '至于',\n",
              " '良好',\n",
              " '若',\n",
              " '若是',\n",
              " '若非',\n",
              " '范围',\n",
              " '莫若',\n",
              " '获得',\n",
              " '虽',\n",
              " '虽则',\n",
              " '虽然',\n",
              " '虽说',\n",
              " '行为',\n",
              " '行动',\n",
              " '表明',\n",
              " '表示',\n",
              " '被',\n",
              " '要',\n",
              " '要不',\n",
              " '要不是',\n",
              " '要不然',\n",
              " '要么',\n",
              " '要是',\n",
              " '要求',\n",
              " '规定',\n",
              " '觉得',\n",
              " '认为',\n",
              " '认真',\n",
              " '认识',\n",
              " '让',\n",
              " '许多',\n",
              " '论',\n",
              " '设使',\n",
              " '设若',\n",
              " '该',\n",
              " '说明',\n",
              " '诸位',\n",
              " '谁',\n",
              " '谁知',\n",
              " '赶',\n",
              " '起',\n",
              " '起来',\n",
              " '起见',\n",
              " '趁',\n",
              " '趁着',\n",
              " '越是',\n",
              " '跟',\n",
              " '转动',\n",
              " '转变',\n",
              " '转贴',\n",
              " '较',\n",
              " '较之',\n",
              " '边',\n",
              " '达到',\n",
              " '迅速',\n",
              " '过',\n",
              " '过去',\n",
              " '过来',\n",
              " '运用',\n",
              " '还是',\n",
              " '还有',\n",
              " '这',\n",
              " '这个',\n",
              " '这么',\n",
              " '这么些',\n",
              " '这么样',\n",
              " '这么点儿',\n",
              " '这些',\n",
              " '这会儿',\n",
              " '这儿',\n",
              " '这就是说',\n",
              " '这时',\n",
              " '这样',\n",
              " '这点',\n",
              " '这种',\n",
              " '这边',\n",
              " '这里',\n",
              " '这麽',\n",
              " '进入',\n",
              " '进步',\n",
              " '进而',\n",
              " '进行',\n",
              " '连',\n",
              " '连同',\n",
              " '适应',\n",
              " '适当',\n",
              " '适用',\n",
              " '逐步',\n",
              " '逐渐',\n",
              " '通常',\n",
              " '通过',\n",
              " '造成',\n",
              " '遇到',\n",
              " '遭到',\n",
              " '避免',\n",
              " '那',\n",
              " '那个',\n",
              " '那么',\n",
              " '那么些',\n",
              " '那么样',\n",
              " '那些',\n",
              " '那会儿',\n",
              " '那儿',\n",
              " '那时',\n",
              " '那样',\n",
              " '那边',\n",
              " '那里',\n",
              " '那麽',\n",
              " '部分',\n",
              " '鄙人',\n",
              " '采取',\n",
              " '里面',\n",
              " '重大',\n",
              " '重新',\n",
              " '重要',\n",
              " '鉴于',\n",
              " '问题',\n",
              " '防止',\n",
              " '阿',\n",
              " '附近',\n",
              " '限制',\n",
              " '除',\n",
              " '除了',\n",
              " '除此之外',\n",
              " '除非',\n",
              " '随',\n",
              " '随着',\n",
              " '随著',\n",
              " '集中',\n",
              " '需要',\n",
              " '非但',\n",
              " '非常',\n",
              " '非徒',\n",
              " '靠',\n",
              " '顺',\n",
              " '顺着',\n",
              " '首先',\n",
              " '高兴',\n",
              " '是不是']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "syn = wordnet.synsets('cookbook')[0]\n",
        "syn.name()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YoJFjP6YMsx7",
        "outputId": "f5738953-853e-4f19-8fc6-c6ce9b0ac3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cookbook.n.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn.definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1OdRP4-vNLuB",
        "outputId": "170e8c06-1162-487c-8783-5810d58eebfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a book of recipes and cooking directions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet.synsets('cookbook.n.01')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dClaPV1MNSjY",
        "outputId": "421b64f4-cc72-4252-8af8-a1bba47a3b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet.synsets('cooking')[0].examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5LtpWlgNdRB",
        "outputId": "b7f46ec2-5ba4-4baa-9750-f77a9213bd82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cooking can be a great art',\n",
              " 'people are needed who have experience in cookery',\n",
              " 'he left the preparation of meals to his wife']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn.hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DScYJA_9NtZ3",
        "outputId": "96642c5f-3434-4168-f86f-c576b08efb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('reference_book.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn.hypernyms()[0].hyponyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk-zU9SsOBlt",
        "outputId": "376f6623-125d-4ed9-8fce-070bd3fe3956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('annual.n.02'),\n",
              " Synset('atlas.n.02'),\n",
              " Synset('cookbook.n.01'),\n",
              " Synset('directory.n.01'),\n",
              " Synset('encyclopedia.n.01'),\n",
              " Synset('handbook.n.01'),\n",
              " Synset('instruction_book.n.01'),\n",
              " Synset('source_book.n.01'),\n",
              " Synset('wordbook.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}